llm 성능평가 지표 비교 (BLEU, ROUGE, BERTScore)
- 간단 설명

  - **BLEU (Bilingual Evaluation Understudy)**  
  정답 문장과 생성 문장 사이의 **n-gram 정밀도**(precision)를 기반으로 평가합니다. 일반적으로 BLEU-4 점수가 가장 많이 사용됩니다.

  - **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**  
  참조 문장 기준으로 얼마나 많이 **재현(Recall)** 했는지를 평가합니다. ROUGE-1, ROUGE-2, ROUGE-L 등이 사용됩니다.

  - **BERTScore**  
  BERT 또는 다른 사전학습 모델로 문장 임베딩을 생성하고, 문맥을 반영할 수 있어 의미적으로 유사한 표현에도 강합니다.

| 지표         | 평가 기준             | 주요 특징                                         | 장점                                                    | 단점                                                       |
|--------------|------------------------|--------------------------------------------------|---------------------------------------------------------|------------------------------------------------------------|
| **BLEU**     | n-gram Precision       | 생성 문장과 참조 문장의 n-gram 겹침 정도 측정        | 기계 번역 등에서 널리 사용되는 전통적 지표                     | 짧은 문장에 불리, 단어 순서에 민감, 의미 고려 불가              |
| **ROUGE**    | n-gram Recall, LCS     | 참조 문장의 단어를 얼마나 잘 재현했는지 평가         | 요약 성능 측정에 적합, recall 중심                           | 반복 표현에 민감, 단어 단위 평가로 의미 비교 어려움             |
| **BERTScore**| Embedding Similarity   | BERT 임베딩을 활용해 의미 유사도 측정                | 의미 기반 평가 가능, 문장 구조가 달라도 평가 가능             | 계산량 큼, 사전학습 모델에 의존, 언어에 따라 편차 발생 가능       |

